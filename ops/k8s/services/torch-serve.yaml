apiVersion: v1
kind: Service
metadata:
  name: mozhi-api-torchserve-svc
  labels:
    org: mozhi
spec:
  selector:
    app: mozhi-api-torchserve
    tier: backend
  ports:
  - protocol: "TCP"
    name: infer-port
    port: 6543
    targetPort: 6543
  - protocol: "TCP"
    port: 6544
    name: mngmnt-port
    targetPort: 6544
  - protocol: "TCP"
    port: 6545
    name: metriccs-port
    targetPort: 6545
  type: LoadBalancer

---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: mozhi-api-torchserve
  labels:
    org: mozhi
    app: mozhi-api-torchserve
spec:
  selector:
    matchLabels:
      app: mozhi-api-torchserve
      tier: backend
  replicas: 1
  template:
    metadata:
      labels:
        app: mozhi-api-torchserve
        tier: backend
        org: mozhi
    spec:
      containers:
        - name: mozhi-api-torchserve
          imagePullPolicy: Always
          image: "mageswaran1989/mozhi-api-cpu:0.4"
          command: [ "/bin/bash" ]
          args: [ "-c", "torchserve --start --model-store /home/model-server/model-store --models all --ts-config configs/torch_serve_config.properties --foreground" ]
          ports:
            - name: infer-port
              containerPort: 6543
            - name: mngmnt-port
              containerPort: 6544
            - name: metriccs-port
              containerPort: 6545
          resources:
            # You must specify requests for CPU to autoscale
            # based on CPU utilization
            requests:
              cpu: "250m"
---

#apiVersion: autoscaling/v1
#kind: HorizontalPodAutoscaler
#metadata:
#  name: mozhi-api-torchserve-hpa
#spec:
#  scaleTargetRef:
#    apiVersion: apps/v1
#    kind: Deployment
#    name: mozhi-api-torchserve
#  minReplicas: 1
#  maxReplicas: 10
#  targetCPUUtilizationPercentage: 50


