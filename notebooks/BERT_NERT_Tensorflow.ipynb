{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    TF2_WEIGHTS_NAME,\n",
    "    BertConfig,\n",
    "    BertTokenizer,\n",
    "    TFBertForTokenClassification,\n",
    "    create_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN= 128\n",
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BTCH_SIZE = 8\n",
    "EPOCHS = 10\n",
    "BERT_MODEL = 'bert-base-uncased'\n",
    "MODEL_PATH = \"model.bin\"\n",
    "TRAINING_FILE = \"../input/entity-annotated-corpus/ner_dataset.csv\"\n",
    "TOKENIZER = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(text, tags):\n",
    "\n",
    "    tokenized_text = []\n",
    "    target_tags = []\n",
    "\n",
    "    for index, token in enumerate(text):\n",
    "\n",
    "        encoded_token = TOKENIZER.encode(\n",
    "            token,\n",
    "            add_special_tokens = False\n",
    "        )\n",
    "\n",
    "        encoded_token_len = len(encoded_token)\n",
    "\n",
    "        tokenized_text.extend(encoded_token)\n",
    "        target_tags.extend([tags[index]] * encoded_token_len)\n",
    "\n",
    "    #truncation\n",
    "    tokenized_text = tokenized_text[: MAX_LEN - 2]\n",
    "    target_tags = target_tags[: MAX_LEN - 2]\n",
    "\n",
    "    #[101] = [CLS] , [102] = [SEP]\n",
    "    tokenized_text = [101] + tokenized_text + [102]\n",
    "    target_tags = [0] + target_tags + [0]\n",
    "    attention_mask = [1] * len(tokenized_text)\n",
    "    token_type_ids = [0] * len(tokenized_text)\n",
    "\n",
    "    #padding\n",
    "    padding_len = int(MAX_LEN - len(tokenized_text))\n",
    "\n",
    "    tokenized_text = tokenized_text + ([0] * padding_len)\n",
    "    target_tags = target_tags + ([0] * padding_len)\n",
    "    attention_mask = attention_mask + ([0] * padding_len)\n",
    "    token_type_ids = token_type_ids + ([0] * padding_len)\n",
    "\n",
    "    return (tokenized_text, target_tags, attention_mask,  token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RetrieveSentence(object):\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        function = lambda s: [(w, p, t) for w, p, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"POS\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sentence #\").apply(function)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def retrieve(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data = pd.read_csv(TRAINING_FILE,sep=\",\",encoding=\"latin1\").fillna(method='ffill')\n",
    "Sentences = RetrieveSentence(df_data)\n",
    "\n",
    "sentences_list = [\" \".join([s[0] for s in sent]) for sent in Sentences.sentences]\n",
    "labels = [ [s[2] for s in sent] for sent in Sentences.sentences]\n",
    "\n",
    "tags_2_val = list(set(df_data[\"Tag\"]))\n",
    "tag_2_idx = {t: i for i, t in enumerate(tags_2_val)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0,\n",
       " 'B-tim': 1,\n",
       " 'B-geo': 2,\n",
       " 'I-art': 3,\n",
       " 'B-eve': 4,\n",
       " 'I-eve': 5,\n",
       " 'I-gpe': 6,\n",
       " 'B-org': 7,\n",
       " 'B-nat': 8,\n",
       " 'B-art': 9,\n",
       " 'I-tim': 10,\n",
       " 'I-nat': 11,\n",
       " 'I-geo': 12,\n",
       " 'B-per': 13,\n",
       " 'B-gpe': 14,\n",
       " 'I-org': 15,\n",
       " 'I-per': 16}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_2_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 14, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_labels = [[tag_2_idx.get(l) for l in lab] for lab in labels]\n",
    "sentences_list = [sent.split() for sent in sentences_list]\n",
    "\n",
    "del labels[41770]\n",
    "del sentences_list[41770]\n",
    "del id_labels[41770]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = []\n",
    "encoded_labels = []\n",
    "attention_masks = []\n",
    "token_type_ids = []\n",
    "\n",
    "for i in range(len(sentences_list)):\n",
    "\n",
    "    text, labels, att_mask, tok_type = get_train_data(text = sentences_list[i], tags = id_labels[i])\n",
    "    encoded_text.append(text)\n",
    "    encoded_labels.append(labels)\n",
    "    attention_masks.append(att_mask)\n",
    "    token_type_ids.append(tok_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#attention_masks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoded_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_text = np.array(encoded_text)\n",
    "encoded_labels = np.array(encoded_labels)\n",
    "attention_masks = np.array(attention_masks)\n",
    "token_type_ids = np.array(token_type_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, Y_train, Y_valid = train_test_split(encoded_text, encoded_labels, random_state=20, test_size=0.1)\n",
    "Mask_train, Mask_valid, Token_ids_train, Token_ids_valid = train_test_split(attention_masks,token_type_ids ,random_state=20, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X train shape:  (43162, 128)\n",
      "X_valid shape:  (4796, 128)\n",
      "Y_train shape:  (43162, 128)\n",
      "Y_valid:  (4796, 128)\n",
      "Mask_train shape:  (43162, 128)\n",
      "Mask_valid shape:  (4796, 128)\n",
      "Token_ids_valid shape:  (4796, 128)\n",
      "Token_ids_train shape:  (43162, 128)\n"
     ]
    }
   ],
   "source": [
    "print(\"X train shape: \", X_train.shape)\n",
    "print(\"X_valid shape: \", X_valid.shape)\n",
    "print(\"Y_train shape: \", Y_train.shape)\n",
    "print(\"Y_valid: \", Y_valid.shape)\n",
    "print(\"Mask_train shape: \", Mask_train.shape)\n",
    "print(\"Mask_valid shape: \", Mask_valid.shape)\n",
    "print(\"Token_ids_valid shape: \", Token_ids_valid.shape)\n",
    "print(\"Token_ids_train shape: \", Token_ids_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_to_features(input_ids,attention_masks,token_type_ids,y):\n",
    "  return {\"input_ids\": input_ids,\n",
    "          \"attention_mask\": attention_masks,\n",
    "          \"token_type_ids\": token_type_ids},y\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train,Mask_train,Token_ids_train,Y_train)).map(example_to_features).shuffle(1000).batch(32)\n",
    "test_ds=tf.data.Dataset.from_tensor_slices((X_valid,Mask_valid,Token_ids_valid,Y_valid)).map(example_to_features).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertForTokenClassification: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['dropout_75', 'classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = BertConfig.from_pretrained(BERT_MODEL,num_labels=len(tags_2_val))\n",
    "model = TFBertForTokenClassification.from_pretrained(BERT_MODEL, from_pt=bool(\".bin\" in BERT_MODEL), config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    " model.layers[-1].activation = tf.keras.activations.softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_bert_for_token_classification_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bert (TFBertMainLayer)       multiple                  109482240 \n",
      "_________________________________________________________________\n",
      "dropout_75 (Dropout)         multiple                  0         \n",
      "_________________________________________________________________\n",
      "classifier (Dense)           multiple                  13073     \n",
      "=================================================================\n",
      "Total params: 109,495,313\n",
      "Trainable params: 109,495,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1349/1349 [==============================] - 785s 582ms/step - loss: 1.9672 - accuracy: 0.9625 - val_loss: 1.9675 - val_accuracy: 0.9620\n",
      "Epoch 2/3\n",
      "1349/1349 [==============================] - 780s 578ms/step - loss: 1.9664 - accuracy: 0.9631 - val_loss: 1.9675 - val_accuracy: 0.9620\n",
      "Epoch 3/3\n",
      "1349/1349 [==============================] - 778s 577ms/step - loss: 1.9664 - accuracy: 0.9631 - val_loss: 1.9675 - val_accuracy: 0.9620\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_ds, epochs=3, validation_data=test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"Hi , my name is Bob and I live in England\"\n",
    "inputs = TOKENIZER(sentence, return_tensors=\"tf\")\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "inputs[\"labels\"] = tf.reshape(tf.constant([1] * tf.size(input_ids).numpy()), (-1, tf.size(input_ids))) # Batch size 1\n",
    "output = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(13,), dtype=float32, numpy=\n",
       " array([2.9295006, 2.9295006, 2.9295006, 2.9295006, 2.9295006, 2.9295006,\n",
       "        2.9295006, 2.9295006, 2.9295006, 2.9295006, 2.9295006, 2.9295006,\n",
       "        2.9295006], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 13, 17), dtype=float32, numpy=\n",
       " array([[[1.0000000e+00, 3.7605066e-09, 5.1076641e-09, 7.5045525e-09,\n",
       "          3.3244767e-09, 3.5249690e-09, 3.2716247e-09, 4.2896966e-09,\n",
       "          4.0232000e-09, 3.5823919e-09, 2.7173985e-09, 3.7711008e-09,\n",
       "          3.6975756e-09, 5.0502962e-09, 2.8892921e-09, 5.4763913e-09,\n",
       "          3.2028369e-09],\n",
       "         [1.0000000e+00, 3.7605212e-09, 5.1076836e-09, 7.5045667e-09,\n",
       "          3.3244829e-09, 3.5249690e-09, 3.2716372e-09, 4.2897046e-09,\n",
       "          4.0232151e-09, 3.5824055e-09, 2.7174036e-09, 3.7711079e-09,\n",
       "          3.6975900e-09, 5.0503060e-09, 2.8893030e-09, 5.4764224e-09,\n",
       "          3.2028429e-09],\n",
       "         [1.0000000e+00, 3.7605137e-09, 5.1076738e-09, 7.5045667e-09,\n",
       "          3.3244767e-09, 3.5249623e-09, 3.2716310e-09, 4.2897046e-09,\n",
       "          4.0232000e-09, 3.5823919e-09, 2.7173932e-09, 3.7711008e-09,\n",
       "          3.6975756e-09, 5.0502869e-09, 2.8892975e-09, 5.4764016e-09,\n",
       "          3.2028369e-09],\n",
       "         [1.0000000e+00, 3.7605212e-09, 5.1076836e-09, 7.5045667e-09,\n",
       "          3.3244767e-09, 3.5249690e-09, 3.2716372e-09, 4.2897046e-09,\n",
       "          4.0232075e-09, 3.5823986e-09, 2.7173985e-09, 3.7711079e-09,\n",
       "          3.6975831e-09, 5.0503060e-09, 2.8893030e-09, 5.4764118e-09,\n",
       "          3.2028369e-09],\n",
       "         [1.0000000e+00, 3.7605212e-09, 5.1076738e-09, 7.5045667e-09,\n",
       "          3.3244703e-09, 3.5249690e-09, 3.2716310e-09, 4.2897046e-09,\n",
       "          4.0232000e-09, 3.5823919e-09, 2.7173932e-09, 3.7711008e-09,\n",
       "          3.6975756e-09, 5.0502962e-09, 2.8893030e-09, 5.4764118e-09,\n",
       "          3.2028369e-09],\n",
       "         [1.0000000e+00, 3.7605212e-09, 5.1076738e-09, 7.5045667e-09,\n",
       "          3.3244767e-09, 3.5249623e-09, 3.2716310e-09, 4.2897046e-09,\n",
       "          4.0232000e-09, 3.5823919e-09, 2.7173985e-09, 3.7711079e-09,\n",
       "          3.6975831e-09, 5.0502962e-09, 2.8893030e-09, 5.4764118e-09,\n",
       "          3.2028369e-09],\n",
       "         [1.0000000e+00, 3.7605137e-09, 5.1076738e-09, 7.5045667e-09,\n",
       "          3.3244767e-09, 3.5249623e-09, 3.2716310e-09, 4.2897046e-09,\n",
       "          4.0232000e-09, 3.5823919e-09, 2.7173932e-09, 3.7711008e-09,\n",
       "          3.6975756e-09, 5.0502962e-09, 2.8893030e-09, 5.4764118e-09,\n",
       "          3.2028369e-09],\n",
       "         [1.0000000e+00, 3.7605212e-09, 5.1076836e-09, 7.5045667e-09,\n",
       "          3.3244767e-09, 3.5249690e-09, 3.2716310e-09, 4.2897046e-09,\n",
       "          4.0232075e-09, 3.5823919e-09, 2.7173985e-09, 3.7711079e-09,\n",
       "          3.6975831e-09, 5.0502962e-09, 2.8893030e-09, 5.4764118e-09,\n",
       "          3.2028369e-09],\n",
       "         [1.0000000e+00, 3.7605212e-09, 5.1076738e-09, 7.5045667e-09,\n",
       "          3.3244767e-09, 3.5249690e-09, 3.2716372e-09, 4.2897046e-09,\n",
       "          4.0232000e-09, 3.5823919e-09, 2.7173985e-09, 3.7711079e-09,\n",
       "          3.6975831e-09, 5.0502962e-09, 2.8893030e-09, 5.4764118e-09,\n",
       "          3.2028369e-09],\n",
       "         [1.0000000e+00, 3.7605212e-09, 5.1076738e-09, 7.5045667e-09,\n",
       "          3.3244703e-09, 3.5249623e-09, 3.2716310e-09, 4.2897046e-09,\n",
       "          4.0232000e-09, 3.5823919e-09, 2.7173932e-09, 3.7711008e-09,\n",
       "          3.6975831e-09, 5.0502962e-09, 2.8893030e-09, 5.4764118e-09,\n",
       "          3.2028309e-09],\n",
       "         [1.0000000e+00, 3.7605137e-09, 5.1076738e-09, 7.5045525e-09,\n",
       "          3.3244767e-09, 3.5249623e-09, 3.2716310e-09, 4.2897046e-09,\n",
       "          4.0232000e-09, 3.5823919e-09, 2.7173932e-09, 3.7711079e-09,\n",
       "          3.6975756e-09, 5.0502869e-09, 2.8892975e-09, 5.4764118e-09,\n",
       "          3.2028369e-09],\n",
       "         [1.0000000e+00, 3.7605066e-09, 5.1076738e-09, 7.5045525e-09,\n",
       "          3.3244703e-09, 3.5249623e-09, 3.2716247e-09, 4.2897046e-09,\n",
       "          4.0232000e-09, 3.5823919e-09, 2.7173881e-09, 3.7711008e-09,\n",
       "          3.6975756e-09, 5.0502869e-09, 2.8892975e-09, 5.4764118e-09,\n",
       "          3.2028309e-09],\n",
       "         [1.0000000e+00, 3.7605137e-09, 5.1076738e-09, 7.5045525e-09,\n",
       "          3.3244767e-09, 3.5249690e-09, 3.2716247e-09, 4.2897046e-09,\n",
       "          4.0232000e-09, 3.5823919e-09, 2.7173985e-09, 3.7711008e-09,\n",
       "          3.6975756e-09, 5.0502962e-09, 2.8892975e-09, 5.4764016e-09,\n",
       "          3.2028369e-09]]], dtype=float32)>)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.0'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
